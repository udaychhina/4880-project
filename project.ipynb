{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data\n",
    "\n",
    "The data is a list of article metadata from NYT for the month of January through\n",
    "the years 2013 to 2024. The data is in JSON format. \n",
    "\n",
    "Our goal is to import all the saved data and create a pandas DataFrame with it.\n",
    "This will let us analyze the data and answer questions like:\n",
    "\n",
    "- Trends in article topics over the last 10 years\n",
    "- Most popular authors\n",
    "- Most popular sections\n",
    "- Most popular keywords\n",
    "- Most popular articles\n",
    "- and so on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "config = dotenv_values(\".env\")\n",
    "\n",
    "API_KEY = config['API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 7337 articles total.\n"
     ]
    }
   ],
   "source": [
    "def get_nyt_articles(year, month):\n",
    "    url = f'https://api.nytimes.com/svc/archive/v1/{year}/{month}.json?api-key={API_KEY}'\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    # return only the articles. The response object contains metadata as well.\n",
    "    return response.json()['response']['docs']\n",
    "\n",
    "def build_nyt_archive():\n",
    "    articles = []\n",
    "    month = 1\n",
    "    for year in range(2014, 2025):\n",
    "        articles.extend(get_nyt_articles(year, month))\n",
    "        print(f'Fetched {len(articles)} articles total.')\n",
    "        time.sleep(20)\n",
    "    return articles\n",
    "\n",
    "def save_nyt_archive(articles):\n",
    "    # check if the data folder exists, if not, create it\n",
    "    if not os.path.exists('data'):\n",
    "        os.makedirs('data')\n",
    "    with open('data/nyt_archive_2014_2024_jan.json', 'w') as f:\n",
    "        json.dump(articles, f)\n",
    "\n",
    "articles = build_nyt_archive()\n",
    "save_nyt_archive(articles)\n",
    "print('NYT archive saved to nyt_archive_2013_2023_jan.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65325\n",
      "{'abstract': 'The Emancipation Proclamation evolved during the Civil War years, as did the thinking of its author.', 'web_url': 'https://opinionator.blogs.nytimes.com/2012/12/31/abraham-lincoln-and-the-emancipation-proclamation/', 'snippet': 'The Emancipation Proclamation evolved during the Civil War years, as did the thinking of its author.', 'lead_paragraph': 'In an op-ed, Eric Foner writes:', 'source': 'The New York Times', 'multimedia': [], 'headline': {'main': 'Abraham Lincoln and the Emancipation Proclamation', 'kicker': 'Opinionator', 'content_kicker': None, 'print_headline': '', 'name': None, 'seo': None, 'sub': None}, 'keywords': [{'name': 'subject', 'value': 'Civil War (US) (1861-65)', 'rank': 1, 'major': 'N'}, {'name': 'subject', 'value': 'Emancipation Proclamation (1863)', 'rank': 2, 'major': 'N'}, {'name': 'subject', 'value': 'Slavery', 'rank': 3, 'major': 'N'}, {'name': 'persons', 'value': 'Lincoln, Abraham', 'rank': 4, 'major': 'N'}], 'pub_date': '2013-01-01T00:05:49+0000', 'document_type': 'article', 'news_desk': '', 'section_name': 'Opinion', 'byline': {'original': 'By The Editors', 'person': [], 'organization': 'The Editors'}, 'type_of_material': 'News', '_id': 'nyt://article/2254bd00-ee83-5775-ab7e-ddfc8ff85d2a', 'word_count': 141, 'uri': 'nyt://article/2254bd00-ee83-5775-ab7e-ddfc8ff85d2a'}\n"
     ]
    }
   ],
   "source": [
    "def load_json():\n",
    "    with open('data/nyt_archive_2013_2024_jan.json', 'r') as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "articles = load_json()\n",
    "print(len(articles))\n",
    "print(articles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['abstract', 'web_url', 'snippet', 'lead_paragraph', 'source', 'multimedia', 'headline', 'keywords', 'pub_date', 'document_type', 'news_desk', 'section_name', 'byline', 'type_of_material', '_id', 'word_count', 'uri'])\n",
      "dict_keys(['main', 'kicker', 'content_kicker', 'print_headline', 'name', 'seo', 'sub'])\n",
      "dict_keys(['name', 'value', 'rank', 'major'])\n",
      "0\n",
      "[{'name': 'subject', 'value': 'Civil War (US) (1861-65)', 'rank': 1, 'major': 'N'}, {'name': 'subject', 'value': 'Emancipation Proclamation (1863)', 'rank': 2, 'major': 'N'}, {'name': 'subject', 'value': 'Slavery', 'rank': 3, 'major': 'N'}, {'name': 'persons', 'value': 'Lincoln, Abraham', 'rank': 4, 'major': 'N'}]\n",
      "{'main': 'Abraham Lincoln and the Emancipation Proclamation', 'kicker': 'Opinionator', 'content_kicker': None, 'print_headline': '', 'name': None, 'seo': None, 'sub': None}\n",
      "{'original': 'By The Editors', 'person': [], 'organization': 'The Editors'}\n"
     ]
    }
   ],
   "source": [
    "# Exploration\n",
    "\n",
    "# Each article is a dictionary with multiple keys. Some of the values are\n",
    "# dictionaries themselves. For example, the 'headline' key has a dictionary\n",
    "# as its value. The 'keywords' key has a list of dictionaries as its value.\n",
    "\n",
    "# keys of the articles dictionary\n",
    "print(articles[0].keys())\n",
    "\n",
    "# keys of the headline dictionary\n",
    "print(articles[0]['headline'].keys())\n",
    "\n",
    "# keys of the first keyword dictionary\n",
    "print(articles[0]['keywords'][0].keys())\n",
    "\n",
    "# multimedia is a list of dictionaries for the multimedia content of the article\n",
    "# count of the multimedia content of the first article\n",
    "print(len(articles[0]['multimedia']))\n",
    "\n",
    "# the data I think we should keep from the articles dictionary is:\n",
    "# abstract, byline (Author Name), document_type, headline, keywords, news_desk, section_name, word_count, \n",
    "\n",
    "# print the data in the keywords field\n",
    "print(articles[0]['keywords'])\n",
    "\n",
    "# print the data in the headline field\n",
    "print(articles[0]['headline'])\n",
    "\n",
    "# print the data in the byline field\n",
    "print(articles[0]['byline'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating dataframes\n",
    "\n",
    "We will create a pandas DataFrame from the data based on the fields we've\n",
    "decided to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def extract_main_author(byline):\n",
    "    \"\"\"Extracts the name of the person with rank 1 from the byline.\"\"\"\n",
    "    if 'person' in byline and byline['person']:\n",
    "        # Look for the person with rank 1\n",
    "        for person in byline['person']:\n",
    "            if person.get('rank') == 1:\n",
    "                # Format the name as 'Firstname Lastname'\n",
    "                return f\"{person.get('firstname', '')} {person.get('lastname', '')}\".strip()\n",
    "    # If no rank 1 person is found, return NaN\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "def extract_keywords_with_subcategories(article):\n",
    "    \"\"\"Extracts keywords and keeps the first rank as main and second rank as subcategory.\"\"\"\n",
    "    keyword_dict = {}\n",
    "    \n",
    "    # Initialize lists to hold keywords by type\n",
    "    subjects = []\n",
    "    organizations = []\n",
    "    glocations = []\n",
    "    persons = []\n",
    "\n",
    "    # Iterate over keywords and categorize them\n",
    "    for keyword in article.get('keywords', []):\n",
    "        keyword_type = keyword['name']\n",
    "        keyword_value = keyword['value']\n",
    "        \n",
    "        # Append the keyword to the appropriate list\n",
    "        if keyword_type == 'subject':\n",
    "            subjects.append(keyword_value)\n",
    "        elif keyword_type == 'organizations':\n",
    "            organizations.append(keyword_value)\n",
    "        elif keyword_type == 'glocations':\n",
    "            glocations.append(keyword_value)\n",
    "        elif keyword_type == 'persons':\n",
    "            keyword_dict['person'] = keyword_value\n",
    "\n",
    "    # Assign the first and second ranked keywords for each type\n",
    "    if subjects:\n",
    "        keyword_dict['subject'] = subjects[0]  # First ranked subject\n",
    "        keyword_dict['subject_subcategory'] = subjects[1] if len(subjects) > 1 else np.nan  # Second ranked subject\n",
    "\n",
    "    if organizations:\n",
    "        keyword_dict['organization'] = organizations[0]  # First ranked organization\n",
    "        keyword_dict['organization_subcategory'] = organizations[1] if len(organizations) > 1 else np.nan  # Second ranked organization\n",
    "\n",
    "    if glocations:\n",
    "        keyword_dict['glocation'] = glocations[0]  # First ranked glocation\n",
    "        keyword_dict['glocation_subcategory'] = glocations[1] if len(glocations) > 1 else np.nan  # Second ranked glocation\n",
    "\n",
    "    if persons:\n",
    "        keyword_dict['person'] = persons[0]\n",
    "        keyword_dict['person_subcategory'] = persons[1] if len(persons) > 1 else np.nan\n",
    "\n",
    "    return keyword_dict\n",
    "\n",
    "articles_data = []\n",
    "for article in articles:\n",
    "    article_data = {\n",
    "        'headline': article['headline']['main'],\n",
    "        'pub_date': article['pub_date'],\n",
    "        'document_type': article['document_type'],\n",
    "        'word_count': article.get('word_count', 0),\n",
    "        'news_desk': article.get('news_desk'),\n",
    "        'section_name': article.get('section_name'),\n",
    "        'type_of_material': article.get('type_of_material'),\n",
    "        'multimedia_count': len(article.get('multimedia', [])),\n",
    "        'author': extract_main_author(article.get('byline', {})),\n",
    "    }\n",
    "    # Add the keywords with subcategories\n",
    "    article_data.update(extract_keywords_with_subcategories(article))\n",
    "\n",
    "    articles_data.append(article_data)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(articles_data)\n",
    "\n",
    "df.to_csv('data/raw.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the null values in each row and add a new column to the dataframe for\n",
    "# the count of null values.\n",
    "df['null_count'] = df.isnull().sum(axis=1)\n",
    "\n",
    "# sort the dataframe by the number of null values in each row\n",
    "df = df.sort_values('null_count', ascending=True)\n",
    "\n",
    "# convert the all entries in the whole dataframe to title case\n",
    "df = df.apply(lambda x: x.str.title() if x.dtype == \"object\" else x)\n",
    "\n",
    "# drop the rows with more than 2 null values\n",
    "df = df[df['null_count'] <= 2]\n",
    "\n",
    "# drop the null_count column\n",
    "df = df.drop(columns='null_count')\n",
    "\n",
    "# save the cleaned data to a new csv file\n",
    "df.to_csv('data/cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date cleaning\n",
    "\n",
    "The date column is a string. We can use regex to extract the year, month, and\n",
    "day from the date and put them in their own columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date cleaning\n",
    "# convert the pub_date column to a datetime object\n",
    "df['pub_date'] = pd.to_datetime(df['pub_date'])\n",
    "\n",
    "# get the year, month, and day from the pub_date column\n",
    "df['year'] = df['pub_date'].dt.year\n",
    "df['month'] = df['pub_date'].dt.month\n",
    "df['day'] = df['pub_date'].dt.day\n",
    "\n",
    "# drop the pub_date column\n",
    "df = df.drop('pub_date', axis=1)\n",
    "\n",
    "# save the cleaned data to a new CSV file\n",
    "df.to_csv('data/cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Headline Apostrophe Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning text encoding in the 'headline' column\n",
    "\n",
    "df['headline'] = df['headline'].str.replace(r'[\\u2018\\u2019]', \"'\", regex=True)\n",
    "\n",
    "df['headline'] = df['headline'].str.replace(\"'S \", \"'s \")\n",
    "\n",
    "df.to_csv('data/cleaned.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persons cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['person'] = df['person'].str.replace(r\"\\s*\\(.*\\)\", \"\", regex=True)\n",
    "\n",
    "# # Switch from 'Lastname, Firstname MiddleInitial' to 'Firstname MiddleInitial Lastname'\n",
    "df['person'] = df['person'].str.replace(\n",
    "    r\"(\\w+),\\s*(\\w+)(\\s+\\w)?\", r\"\\2\\3 \\1\", regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keywords cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['subject'] = df['subject'].str.replace(r\"\\s*\\(.*\\)\", \"\", regex=True)\n",
    "\n",
    "df['subject_subcategory'] = df['subject_subcategory'].str.replace(r\"\\s*\\(.*\\)\", \"\", regex=True)\n",
    "\n",
    "df['organization'] = df['organization'].str.replace(r\"\\s*\\(.*\\)\", \"\", regex=True)\n",
    "\n",
    "df['organization_subcategory'] = df['organization_subcategory'].str.replace(r\"\\s*\\(.*\\)\", \"\", regex=True)\n",
    "\n",
    "df['type_of_material'] = df['type_of_material'].str.replace(r\"\\s*\\(.*\\)\", \"\", regex=True)\n",
    "\n",
    "\n",
    "\n",
    "df.to_csv('data/cleaned.csv', encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
