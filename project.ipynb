{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data\n",
    "\n",
    "The data is a list of article metadata from NYT for the month of January through\n",
    "the years 2013 to 2024. The data is in JSON format. \n",
    "\n",
    "Our goal is to import all the saved data and create a pandas DataFrame with it.\n",
    "This will let us analyze the data and answer questions like:\n",
    "\n",
    "- Trends in article topics over the last 10 years\n",
    "- Most popular authors\n",
    "- Most popular sections\n",
    "- Most popular keywords\n",
    "- Most popular articles\n",
    "- and so on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "config = dotenv_values(\".env\")\n",
    "\n",
    "API_KEY = config['API_KEY']\n",
    "\n",
    "def get_nyt_articles(year, month):\n",
    "    url = f'https://api.nytimes.com/svc/archive/v1/{year}/{month}.json?api-key={API_KEY}'\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    # return only the articles. The response object contains metadata as well.\n",
    "    return response.json()['response']['docs']\n",
    "\n",
    "def build_nyt_archive():\n",
    "    articles = []\n",
    "    month = 1\n",
    "    for year in range(2014, 2025):\n",
    "        articles.extend(get_nyt_articles(year, month))\n",
    "        print(f'Fetched {len(articles)} articles total.')\n",
    "        time.sleep(20)\n",
    "    return articles\n",
    "\n",
    "def save_nyt_archive(articles):\n",
    "    # check if the data folder exists, if not, create it\n",
    "    if not os.path.exists('data'):\n",
    "        os.makedirs('data')\n",
    "    with open('data/nyt_archive_2014_2024_jan.json', 'w') as f:\n",
    "        json.dump(articles, f)\n",
    "\n",
    "articles = build_nyt_archive()\n",
    "save_nyt_archive(articles)\n",
    "print('NYT archive saved to nyt_archive_2013_2023_jan.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57364\n",
      "{'abstract': 'Other than the national championship game, the Rose Bowl offers the most anticipated matchup of the season, but the Capital One Bowl and Fiesta Bowl should also be intriguing.', 'web_url': 'https://www.nytimes.com/2014/01/01/sports/ncaafootball/new-years-day-bowl-games.html', 'snippet': 'Other than the national championship game, the Rose Bowl offers the most anticipated matchup of the season, but the Capital One Bowl and Fiesta Bowl should also be intriguing.', 'lead_paragraph': 'ROSE BOWL', 'print_section': 'B', 'print_page': '10', 'source': 'The New York Times', 'multimedia': [{'rank': 0, 'subtype': 'xlarge', 'caption': None, 'credit': None, 'type': 'image', 'url': 'images/2014/01/01/sports/Y-games/Y-games-articleLarge.jpg', 'height': 318, 'width': 600, 'subType': 'xlarge', 'crop_name': 'articleLarge', 'legacy': {'xlarge': 'images/2014/01/01/sports/Y-games/Y-games-articleLarge.jpg', 'xlargewidth': 600, 'xlargeheight': 318}}, {'rank': 0, 'subtype': 'jumbo', 'caption': None, 'credit': None, 'type': 'image', 'url': 'images/2014/01/01/sports/Y-games/Y-games-jumbo.jpg', 'height': 542, 'width': 1024, 'subType': 'jumbo', 'crop_name': 'jumbo', 'legacy': {}}, {'rank': 0, 'subtype': 'superJumbo', 'caption': None, 'credit': None, 'type': 'image', 'url': 'images/2014/01/01/sports/Y-games/Y-games-superJumbo.jpg', 'height': 1085, 'width': 2048, 'subType': 'superJumbo', 'crop_name': 'superJumbo', 'legacy': {}}, {'rank': 0, 'subtype': 'thumbnail', 'caption': None, 'credit': None, 'type': 'image', 'url': 'images/2014/01/01/sports/Y-games/Y-games-thumbStandard.jpg', 'height': 75, 'width': 75, 'subType': 'thumbnail', 'crop_name': 'thumbStandard', 'legacy': {'thumbnail': 'images/2014/01/01/sports/Y-games/Y-games-thumbStandard.jpg', 'thumbnailwidth': 75, 'thumbnailheight': 75}}, {'rank': 0, 'subtype': 'thumbLarge', 'caption': None, 'credit': None, 'type': 'image', 'url': 'images/2014/01/01/sports/Y-games/Y-games-thumbLarge.jpg', 'height': 150, 'width': 150, 'subType': 'thumbLarge', 'crop_name': 'thumbLarge', 'legacy': {}}], 'headline': {'main': 'Bowl Games to Watch on New Year’s Day', 'kicker': None, 'content_kicker': None, 'print_headline': ' Bowl Games to Watch on New Year&#8217;s Day', 'name': None, 'seo': None, 'sub': None}, 'keywords': [{'name': 'persons', 'value': 'Bierman, Fred', 'rank': 1, 'major': 'N'}, {'name': 'subject', 'value': 'Rose Bowl (Football Game)', 'rank': 2, 'major': 'N'}, {'name': 'subject', 'value': 'Football (College)', 'rank': 3, 'major': 'N'}, {'name': 'organizations', 'value': 'Stanford University', 'rank': 4, 'major': 'N'}, {'name': 'organizations', 'value': 'Michigan State University', 'rank': 5, 'major': 'N'}, {'name': 'subject', 'value': 'Fiesta Bowl', 'rank': 6, 'major': 'N'}, {'name': 'organizations', 'value': 'University of South Carolina', 'rank': 7, 'major': 'N'}, {'name': 'organizations', 'value': 'University of Wisconsin-Madison', 'rank': 8, 'major': 'N'}, {'name': 'organizations', 'value': 'Baylor University', 'rank': 9, 'major': 'N'}, {'name': 'organizations', 'value': 'University of Central Florida', 'rank': 10, 'major': 'N'}], 'pub_date': '2014-01-01T00:00:45+0000', 'document_type': 'article', 'news_desk': 'Sports', 'section_name': 'Sports', 'subsection_name': 'College Football', 'byline': {'original': 'By Fred Bierman', 'person': [{'firstname': 'Fred', 'middlename': None, 'lastname': 'Bierman', 'qualifier': None, 'title': None, 'role': 'reported', 'organization': '', 'rank': 1}], 'organization': None}, 'type_of_material': 'News', '_id': 'nyt://article/b56938ec-95f0-5303-99ff-66542cbe724f', 'word_count': 845, 'uri': 'nyt://article/b56938ec-95f0-5303-99ff-66542cbe724f'}\n"
     ]
    }
   ],
   "source": [
    "def load_json():\n",
    "    with open('data/nyt_archive_2014_2024_jan.json', 'r') as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "articles = load_json()\n",
    "print(len(articles))\n",
    "print(articles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['abstract', 'web_url', 'snippet', 'lead_paragraph', 'print_section', 'print_page', 'source', 'multimedia', 'headline', 'keywords', 'pub_date', 'document_type', 'news_desk', 'section_name', 'subsection_name', 'byline', 'type_of_material', '_id', 'word_count', 'uri'])\n",
      "dict_keys(['main', 'kicker', 'content_kicker', 'print_headline', 'name', 'seo', 'sub'])\n",
      "dict_keys(['name', 'value', 'rank', 'major'])\n",
      "5\n",
      "[{'name': 'persons', 'value': 'Bierman, Fred', 'rank': 1, 'major': 'N'}, {'name': 'subject', 'value': 'Rose Bowl (Football Game)', 'rank': 2, 'major': 'N'}, {'name': 'subject', 'value': 'Football (College)', 'rank': 3, 'major': 'N'}, {'name': 'organizations', 'value': 'Stanford University', 'rank': 4, 'major': 'N'}, {'name': 'organizations', 'value': 'Michigan State University', 'rank': 5, 'major': 'N'}, {'name': 'subject', 'value': 'Fiesta Bowl', 'rank': 6, 'major': 'N'}, {'name': 'organizations', 'value': 'University of South Carolina', 'rank': 7, 'major': 'N'}, {'name': 'organizations', 'value': 'University of Wisconsin-Madison', 'rank': 8, 'major': 'N'}, {'name': 'organizations', 'value': 'Baylor University', 'rank': 9, 'major': 'N'}, {'name': 'organizations', 'value': 'University of Central Florida', 'rank': 10, 'major': 'N'}]\n",
      "{'main': 'Bowl Games to Watch on New Year’s Day', 'kicker': None, 'content_kicker': None, 'print_headline': ' Bowl Games to Watch on New Year&#8217;s Day', 'name': None, 'seo': None, 'sub': None}\n",
      "{'original': 'By Fred Bierman', 'person': [{'firstname': 'Fred', 'middlename': None, 'lastname': 'Bierman', 'qualifier': None, 'title': None, 'role': 'reported', 'organization': '', 'rank': 1}], 'organization': None}\n"
     ]
    }
   ],
   "source": [
    "# Exploration\n",
    "\n",
    "# Each article is a dictionary with multiple keys. Some of the values are\n",
    "# dictionaries themselves. For example, the 'headline' key has a dictionary\n",
    "# as its value. The 'keywords' key has a list of dictionaries as its value.\n",
    "\n",
    "# keys of the articles dictionary\n",
    "print(articles[0].keys())\n",
    "\n",
    "# keys of the headline dictionary\n",
    "print(articles[0]['headline'].keys())\n",
    "\n",
    "# keys of the first keyword dictionary\n",
    "print(articles[0]['keywords'][0].keys())\n",
    "\n",
    "# multimedia is a list of dictionaries for the multimedia content of the article\n",
    "# count of the multimedia content of the first article\n",
    "print(len(articles[0]['multimedia']))\n",
    "\n",
    "# the data I think we should keep from the articles dictionary is:\n",
    "# abstract, byline (Author Name), document_type, headline, keywords, news_desk, section_name, word_count, \n",
    "\n",
    "# print the data in the keywords field\n",
    "print(articles[0]['keywords'])\n",
    "\n",
    "# print the data in the headline field\n",
    "print(articles[0]['headline'])\n",
    "\n",
    "# print the data in the byline field\n",
    "print(articles[0]['byline'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating dataframes\n",
    "\n",
    "We will create a pandas DataFrame from the data based on the fields we've\n",
    "decided to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def extract_keywords_with_subcategories(article):\n",
    "    \"\"\"Extracts keywords and keeps the first rank as main and second rank as subcategory.\"\"\"\n",
    "    keyword_dict = {}\n",
    "    \n",
    "    # Initialize lists to hold keywords by type\n",
    "    subjects = []\n",
    "    organizations = []\n",
    "    glocations = []\n",
    "    persons = []\n",
    "\n",
    "    # Iterate over keywords and categorize them\n",
    "    for keyword in article.get('keywords', []):\n",
    "        keyword_type = keyword['name']\n",
    "        keyword_value = keyword['value']\n",
    "        \n",
    "        # Append the keyword to the appropriate list\n",
    "        if keyword_type == 'subject':\n",
    "            subjects.append(keyword_value)\n",
    "        elif keyword_type == 'organizations':\n",
    "            organizations.append(keyword_value)\n",
    "        elif keyword_type == 'glocations':\n",
    "            glocations.append(keyword_value)\n",
    "        elif keyword_type == 'persons':\n",
    "            keyword_dict['person'] = keyword_value\n",
    "\n",
    "    # Assign the first and second ranked keywords for each type\n",
    "    if subjects:\n",
    "        keyword_dict['subject'] = subjects[0]  # First ranked subject\n",
    "        keyword_dict['subject_subcategory'] = subjects[1] if len(subjects) > 1 else np.nan  # Second ranked subject\n",
    "\n",
    "    if organizations:\n",
    "        keyword_dict['organization'] = organizations[0]  # First ranked organization\n",
    "        keyword_dict['organization_subcategory'] = organizations[1] if len(organizations) > 1 else np.nan  # Second ranked organization\n",
    "\n",
    "    if glocations:\n",
    "        keyword_dict['glocation'] = glocations[0]  # First ranked glocation\n",
    "        keyword_dict['glocation_subcategory'] = glocations[1] if len(glocations) > 1 else np.nan  # Second ranked glocation\n",
    "\n",
    "    if persons:\n",
    "        keyword_dict['person'] = persons[0]\n",
    "        keyword_dict['person_subcategory'] = persons[1] if len(persons) > 1 else np.nan\n",
    "\n",
    "    return keyword_dict\n",
    "\n",
    "# Data processing loop\n",
    "articles_data = []\n",
    "for article in articles:\n",
    "    article_data = {\n",
    "        'headline': article['headline']['main'],\n",
    "        'pub_date': article['pub_date'],\n",
    "        'document_type': article['document_type'],\n",
    "        'word_count': article.get('word_count', 0),\n",
    "        'news_desk': article.get('news_desk'),\n",
    "        'section_name': article.get('section_name'),\n",
    "        'type_of_material': article.get('type_of_material'),\n",
    "        'multimedia_count': len(article.get('multimedia', [])),\n",
    "        'byline': article.get('byline', {}).get('original', np.nan),\n",
    "    }\n",
    "    # Add the keywords with subcategories\n",
    "    article_data.update(extract_keywords_with_subcategories(article))\n",
    "\n",
    "    articles_data.append(article_data)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(articles_data)\n",
    "\n",
    "df.to_csv('data/raw.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the null values in each row and add a new column to the dataframe for\n",
    "# the count of null values.\n",
    "df['null_count'] = df.isnull().sum(axis=1)\n",
    "\n",
    "# sort the dataframe by the number of null values in each row\n",
    "df = df.sort_values('null_count', ascending=True)\n",
    "\n",
    "# convert the all entries in the whole dataframe to title case\n",
    "df = df.apply(lambda x: x.str.title() if x.dtype == \"object\" else x)\n",
    "\n",
    "# save the cleaned data to a new csv file\n",
    "df.to_csv('data/cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data\n",
    "\n",
    "For this next part we are going clean some of the data columns. Everything\n",
    "after multimedia counts is a keyword. We need to make sure we are able to\n",
    "separate them into their own columns so that it's a little clearer to see what\n",
    "is going on. Right now the names of the keywords are the column headers and\n",
    "the values are in the rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the columns of the dataframe\n",
    "#print(df.columns)\n",
    "\n",
    "# let's find out unique values in the subject column and the count.\n",
    "# print(df['subject'].value_counts())\n",
    "\n",
    "# right now, the subject column groups each of the subjects into a single\n",
    "# string, separated by commas. Let's split them into separate columns. We will\n",
    "# name them subject_1, subject_2, subject_3, and so on. This will depend on\n",
    "# the number of subjects for each article. Some have only one subject, while\n",
    "# others have multiple subjects.\n",
    "\n",
    "# print(df['subject'].head())\n",
    "# for i in range(20):\n",
    "#     print(df.iloc[i]['subject'])\n",
    "# print(df.iloc[0]['subject'])\n",
    "# get the maximum number of subjects for an article\n",
    "# max_subjects = df['subject'].str.split(', ')\n",
    "\n",
    "# split_data = max_subjects.dropna().apply(lambda x: [item.strip() for item in x[0].split(',')])\n",
    "# print(split_data)\n",
    "\n",
    "# max_subjects = split_data.apply(len).max()\n",
    "\n",
    "\n",
    "# print(max_subjects)\n",
    "\n",
    "# print(type(max_subjects))\n",
    "# # create a new dataframe with the subject columns\n",
    "# df_subjects = df['subject'].str.split(',', expand=True)\n",
    "\n",
    "# # rename the columns\n",
    "# df_subjects.columns = [f'subject_{i+1}' for i in range(max_subjects)]\n",
    "\n",
    "# # combine the two dataframes\n",
    "# df = pd.concat([df, df_subjects], axis=1)\n",
    "\n",
    "# # drop the original subject column\n",
    "# df = df.drop('subject', axis=1)\n",
    "\n",
    "# print(df.head())\n",
    "\n",
    "df.loc[df.glocations.notna(), 'glocations'] = df.glocations.str.title()\n",
    "\n",
    "\n",
    "# get the count of subjects for each article\n",
    "df['subject_count'] = df['subject'].apply(lambda x: len(x.split(',')) if pd.notna(x) else 0)\n",
    "\n",
    "# get the row with the maximum number of subjects\n",
    "max_subject_row = df['subject_count'].idxmax()\n",
    "\n",
    "# count for the maximum number of subjects\n",
    "# print(df['subject_count'].max())\n",
    "\n",
    "# print the row with the maximum number of subjects\n",
    "\n",
    "# print(df['subject_count'].head())\n",
    "\n",
    "subjects_split = df['subject'].str.split(',', expand=True)\n",
    "\n",
    "subjects_split.columns = [f\"subject_{i+1}\" for i in range(subjects_split.shape[1])]\n",
    "\n",
    "# df_with_subjects.to_csv('data/nyt_archive_2014_2024_jan.csv', index=False)\n",
    "\n",
    "# We will have to do the same thing with the organizations and glocations\n",
    "# columns.\n",
    "\n",
    "# get the count of organizations for each article\n",
    "df['organizations_count'] = df['organizations'].apply(lambda x: len(x.split(',')) if pd.notna(x) else 0)\n",
    "\n",
    "# get the row with the maximum number of organizations\n",
    "max_organizations_row = df['organizations_count'].idxmax()\n",
    "\n",
    "\n",
    "\n",
    "organizations_split = df['organizations'].str.split(',', expand=True)\n",
    "\n",
    "organizations_split.columns = [f\"organization_{i+1}\" for i in range(organizations_split.shape[1])]\n",
    "# df_with_organizations = pd.concat([df, organizations_split], axis=1)\n",
    "\n",
    "# get the count of glocations for each article\n",
    "df['glocations_count'] = df['glocations'].apply(lambda x: len(x.split(',')) if pd.notna(x) else 0)\n",
    "\n",
    "# get the row with the maximum number of glocations\n",
    "max_glocations_row = df['glocations_count'].idxmax()\n",
    "\n",
    "\n",
    "\n",
    "glocations_split = df['glocations'].str.split(',', expand=True)\n",
    "\n",
    "glocations_split.columns = [f\"glocation_{i+1}\" for i in range(glocations_split.shape[1])]\n",
    "\n",
    "\n",
    "df_separated = pd.concat([df, subjects_split, organizations_split, glocations_split], axis=1)\n",
    "\n",
    "df_separated.to_csv('data/nyt_archive_2014_2024_jan.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date cleaning\n",
    "\n",
    "The date column is a string. We can use regex to extract the year, month, and\n",
    "day from the date and put them in their own columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date cleaning\n",
    "# convert the pub_date column to a datetime object\n",
    "df_separated['pub_date'] = pd.to_datetime(df_separated['pub_date'])\n",
    "\n",
    "# get the year, month, and day from the pub_date column\n",
    "df_separated['year'] = df_separated['pub_date'].dt.year\n",
    "df_separated['month'] = df_separated['pub_date'].dt.month\n",
    "df_separated['day'] = df_separated['pub_date'].dt.day\n",
    "\n",
    "# drop the pub_date column\n",
    "df_separated = df_separated.drop('pub_date', axis=1)\n",
    "\n",
    "# save the cleaned data to a new CSV file\n",
    "df_separated.to_csv('data/nyt_archive_2014_2024_jan_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author cleaning\n",
    "\n",
    "The author column is a string. We can use regex to extract the author name\n",
    "from the author column and put it in its own column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            6278\n",
      "The New York Times                          1934\n",
      "By The New York Times                        708\n",
      "By The Editorial Board                       531\n",
      "By The Learning Network                      446\n",
      "                                            ... \n",
      "By Lauren Hard                                 1\n",
      "By Jeremy D. Goodwin                           1\n",
      "By Robert W. Goldfarb                          1\n",
      "By Emily Cochrane and Michael S. Schmidt       1\n",
      "By Claire Moses and Orlando Mayorquin          1\n",
      "Name: byline, Length: 10760, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# author cleaning\n",
    "\n",
    "# get the unique values in the byline column\n",
    "print(df_separated['byline'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      6278\n",
      "The New York Times                    2642\n",
      "The Editorial Board                    531\n",
      "The Learning Network                   446\n",
      "Paul Krugman                           306\n",
      "                                      ... \n",
      "Maryanne Garbowsky                       1\n",
      "Niraj Chokshi and Daniel Victor          1\n",
      "Jordan Rau                               1\n",
      "Amir Ahmadi Arian                        1\n",
      "Claire Moses and Orlando Mayorquin       1\n",
      "Name: byline, Length: 10709, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# remove the 'By ' prefix from the byline column\n",
    "df_separated['byline'] = df_separated['byline'].str.replace('By ', '')\n",
    "\n",
    "# get unique values in the byline column\n",
    "print(df_separated['byline'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename byline column to author\n",
    "df_separated = df_separated.rename(columns={'byline': 'author'})\n",
    "\n",
    "# save the cleaned data to a new CSV file\n",
    "df_separated.to_csv('data/nyt_archive_2014_2024_jan_cleaned.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some articles have multiple authors and some have none. Some have only one\n",
    "# author. The multiple authors are split by commas and the word 'and'. We can\n",
    "# split the authors into separate columns. We will name them author_1,\n",
    "# author_2,\n",
    "# author_3, and so on. This will depend on the number of authors for each \n",
    "# article. \n",
    "\n",
    "# we can do this similar to how we split the subjects, organizations, and\n",
    "# glocations columns.\n",
    "\n",
    "# get the count of authors for each article\n",
    "df_separated['author_count'] = df_separated['author'].apply(lambda x: len(x.split(', | and')) if pd.notna(x) else 0)\n",
    "\n",
    "# get the row with the maximum number of authors\n",
    "max_author_row = df_separated['author_count'].idxmax()\n",
    "\n",
    "# split the authors into separate columns\n",
    "authors_split = df_separated['author'].str.split(', | and', expand=True)\n",
    "\n",
    "authors_split.columns = [f\"author_{i+1}\" for i in range(authors_split.shape[1])]\n",
    "\n",
    "df_separated = pd.concat([df_separated, authors_split], axis=1)\n",
    "\n",
    "# save the cleaned data to a new CSV file\n",
    "df_separated.to_csv('data/nyt_archive_2014_2024_jan_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                headline document_type  \\\n",
      "54440                            52 Places to Go in 2024    multimedia   \n",
      "51354                            52 Places to Go in 2023    multimedia   \n",
      "35824                            52 Places to Go in 2020    multimedia   \n",
      "47321                      52 Places for a Changed World    multimedia   \n",
      "37168  You Don’t Need to Travel to Profit From Intern...       article   \n",
      "\n",
      "       word_count       news_desk  section_name     type_of_material  \\\n",
      "54440           0          Travel        Travel  Interactive Feature   \n",
      "51354           0          Travel        Travel  Interactive Feature   \n",
      "35824           0          Travel        Travel  Interactive Feature   \n",
      "47321           0          Travel        Travel  Interactive Feature   \n",
      "37168         990  SundayBusiness  Business Day                 News   \n",
      "\n",
      "       multimedia_count                 byline persons  \\\n",
      "54440                 5  By The New York Times     NaN   \n",
      "51354                 5                            NaN   \n",
      "35824                 5  By The New York Times     NaN   \n",
      "47321                 5                            NaN   \n",
      "37168                 5           By Liz Moyer     NaN   \n",
      "\n",
      "                                                 subject  ...  \\\n",
      "54440  Travel and Vacations, Two Thousand Twenty Four...  ...   \n",
      "51354  internal-sub-only, Travel and Vacations, Two T...  ...   \n",
      "35824                               Travel and Vacations  ...   \n",
      "47321  Travel and Vacations, Coronavirus Reopenings, ...  ...   \n",
      "37168  Stocks and Bonds, Mutual Funds, Drugs (Pharmac...  ...   \n",
      "\n",
      "            glocation_48               glocation_49       glocation_50  \\\n",
      "54440   Everglades (Fla)                    Morocco            Bolivia   \n",
      "51354            Montana   Quebec Province (Canada)   New Haven (Conn)   \n",
      "35824           Slovenia     Addis Ababa (Ethiopia)            Romania   \n",
      "47321               None                       None               None   \n",
      "37168               None                       None               None   \n",
      "\n",
      "          glocation_51                   glocation_52       glocation_53  \\\n",
      "54440             None                           None               None   \n",
      "51354             None                           None               None   \n",
      "35824   Urbino (Italy)   Glacier National Park (Mont)   Whitefish (Mont)   \n",
      "47321             None                           None               None   \n",
      "37168             None                           None               None   \n",
      "\n",
      "       year month day null_count  \n",
      "54440  2024     1   9         49  \n",
      "51354  2023     1  12         54  \n",
      "35824  2020     1   8         54  \n",
      "47321  2022     1  10         54  \n",
      "37168  2020     1  17         63  \n",
      "\n",
      "[5 rows x 125 columns]\n"
     ]
    }
   ],
   "source": [
    "df_separated['null_count'] = df_separated.isnull().sum(axis=1)\n",
    "\n",
    "df_sorted = df_separated.sort_values('null_count', ascending=True)\n",
    "\n",
    "print(df_sorted.head()) \n",
    "\n",
    "df_less = df_sorted[:15000]\n",
    "\n",
    "df_less.to_csv('data/nyt_archive_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
